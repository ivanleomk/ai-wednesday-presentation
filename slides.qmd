---
title: "What's Next for LLMs"
author: "Ivan Leo"
format: revealjs
---

## Who am I

I am currently a Research Engineer at 567 Labs. We primarily work with companies looking to scale out their machine learning capabilities. We help at all stages of this, from hiring to prototyping and high level design.

I do some open source on the side and help mantain **Instructor** - a python package that makes it easy to use LLMs to extract structured data ( More on this ). I mostly code in Python nowadays but am moving to do more Rust. ( Instructor is releasing a rust package soon! )

## What's New

Last year we saw a lot of experimentation, this year with more funding and interest in the space, the goal has shifted to making them more reliable, consistent and secure.


## Definitions

There are three things that will come up a lot in my talk

1. **Agents** - Systems that are able to make a decision based on some input
2. **Evaluations** - a set of metrics to understand where our system falls short (Eg. Precision and Recall)
3. **Synthethic Data Generation** - Data generated by a LLM that's meant to mimic some form of real data


## Big Picture

How does this come together?

1. As we deploy more LLMs in productions, we'll start using more systems with agentic behaviour.
2. This will be a complex process so we'll start developing more sophisticated evaluations to understand each component of this process.
3. And ultimately when we want to invest time into improving capabilities of specific components of these systems, we'll use synthethic data to make sure our system is robust and reliable.


## Structured Extraction

This means that we need a consistent way to ensure that our systems are reliable and consistent. 

I think Structured Extraction is the way to go when it comes to this, that's what Instructor solves.

![](/images/types.jpeg)


## How does this look like?

It's pretty straightforward, basically you make a function call and get back a validated output

```python
import instructor
from pydantic import BaseModel
from openai import OpenAI


# Define your desired output structure
class UserInfo(BaseModel):
    name: str
    age: int


# Patch the OpenAI client
client = instructor.from_openai(OpenAI())

# Extract structured data from natural language
user_info = client.chat.completions.create(
    model="gpt-3.5-turbo",
    response_model=UserInfo,
    messages=[{"role": "user", "content": "John Doe is 30 years old."}],
)

print(user_info.name)
#> John Doe
print(user_info.age)
#> 30
```

## Tl;Dr

In short, you can express LLM calls as a typed function, not unlike a traditional function call 

```python
def function() -> str:
    return "Hello, World!"
```


## Agents

Instead of encoding in hard logic that is brittle when the input changes, we can instead provide a model with some context and let it make decisions in a fuzzier manner.

This is acceptable for some specific processes and helps in overall adaptability.

## What happened last Year?

![](/images/langchain.jpeg){ width=100% }

## What was the main problem

Libraries such as BabyAGI and Langchain enabled people to chain together complex iterations. But, these agents often lacked conconistency and reliability with the open ended ReACT methodology.

## Sample React Prompt

![](/images/React.png)

## What's the problem with this?

The main problem with the ReAct methodology is that it's too open ended. It's difficult to accurately evaluate where the agent might have gone wrong or if there was a better approach sometimes

In short it was

1. Difficult to evaluate the quality of the agent
2. Difficult to understand the reasoning behind the agent's decisions
3. Difficult to debug and improve the agent

## What's the problem with this?

A lot of these agents would loop for 6 hours, give you a $80 OpenAI Bill and not get anything done.

![](/images/agent_memes.png){ width=50%}

## A better way

Instead, this year we've seen a shift from a view agents as autonomous systems that have full autonomy without guardrails to more complex fuzzy decision making proceses.

![](/images/agent.jpeg)

## Broad Trends

There are a few broad reasons why this is happening

1. They're easier to test - **assert func is called** is much easier to reason about than a complex ReAct Loop
2. It's much easier to scale multi-agent systems (Since they now output structured data)
3. We can integreate them easily with existing systems with significantly less effort

## Evaluations

Evaluations were huge this year. 

Historically it's been difficult to evaluate LLM outputs due to their open-ended nature. Some specific issues that are still unsolved include alignment of styles and hallucinations.

![](/images/llm_as_judge.jpg)

## What's changed

A lot more of insight has been gained in the last year. 

1. **Use Heuristics** : A bigger shift to using heuristics to test systems - Eg. Discord checking to see if the messages produced by their chatbot are casual by verifying if it's all in lowercase
2. **LLM as a Judge is Ok** : LLM as a judge for fuzzy initial checks - eventually you align it with human preferences with manual annotators creating a test/train dataset for you to train models on

## What's changed

3. **Break it up** : A general consensus that you should break up your evaluation into smaller, more manageable pieces. True/False binary labels are much easier to align and consistent across annotators. 

## Synthethic Data

Generating Synthethic Data is easy, generating good synthethic data is hard.

## Why Synthethic Data?

It's not because we're running out of data to train foundation models. 

As we train and deploy models for niche use cases, it's increasingly easier to generate synthethic data to test and validate our implementations (Eg. A model that needs to be able to click on buttons on the web or Retrieval for RAG )

## What's hard about it?

It's extremely difficult to ensure that 

1. Our data is representative of the real world
2. Our data is useful for our use case

A great presentation during the conference was by Vikhyatk, who talked about the moondream model

## Moondream Intro

Moondream is a small vision model, with ~1.5 billion parameters. That's significantly smaller than most models these days.

They trained the model with a significant of synthethic data and shared a lot about their insights in generating the training data. 

The challenge comes when you're prompting the same model for multiple rounds - it tends to converge on the same topics.

## A bit more about Moondream

- Inject unique elements into each prompt
  - Example: Use image alt text for each image
  - Use some form of permutation
- Generate absurd questions to help the model learn when to refuse certain questions
  - Example: Using Mixtral to generate absurd questions
  - Note: Mixtral tends to generate questions about Aliens and Dinosaurs, so always review the generated questions

## Moondream

![](/images/Moondream.png)


## Stuctured Extraction

So let's see a potential workload in action - generating synthethic questions

You have some existing knowledge base that you'd like to generate synthethic questions from so you can test your retrieval pipeline.

## Generating the Questions

```python
class QuestionAnswerPair(BaseModel):
    """
    This model represents a pair of a question generated 
    from a text chunk, its corresponding answer, and the 
    chain of thought leading to the answer. The chain of 
    thought provides insight into how the answer was 
    derived from the question.
    """

    chain_of_thought: str = Field(
        description="The reasoning process leading to the answer."
    )
    question: str = Field(
        description="The generated question from the text chunk."
    )
    answer: str = Field(
        description="The answer to the generated question."
    )
```

## Generating the Questions

```python
client = instructor.from_openai(openai.AsyncOpenAI())

def generate_question(text: str):
    question = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": """You are a world class AI that excels at generating hypothethical search queries. 
                You're about to be given a text snippet and asked to generate a search query which is specific 
                to the specific text chunk that you'll be given. Make sure to use information from the text chunk."""
            },
            {"role": "user", "content": f"Here is the text chunk : {text}"},
        ],
        response_model=QuestionAnswerPair,
        max_retries=3,
    )
    return (question,text)
```

## Quick Experimentation

With a new synthethic dataset, we can now do things such as 

1. Understand the kinds of data that is within our query corpus - what specific methods struggle with different text chunks?
2. Experiment with different system permutations - what acceptable performance/latency/cost is for a given system?


## Evaluating the Retrieval

Evaluating the retrieval is easy since we can quickly generate a large dataset of question to text pairs. This allows us to test metrics such as recall, precision or mrr across a large number of queries easily.

![](/images/eval_metrics.png)

## Deploying it in production

Deploying it as a function call in production is easy 

```python
import instructor
from openai import OpenAI

client = instructor.from_openai(OpenAI())

class QueryPlan(BaseModel):
    sub_queries:list[str]

    def execute_query():
        ## Retrieval Logic here
        return ""

def generate_query_plan():
    return client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "user", "content": "Generate a query plan for the user query of {query}"}
        ],
        response_model=QueryPlan,
    )
```

## Monitoring 

This gives us a lot of potential ways to monitor the performance of our system. 

1. We can see the distribution of user queries - how different are the synthethic questions from the actual user queries?
2. We can see if our language model is able to retrieve the relevant chunks ( this needs domain expertise ) and answer questions such as do we need more text material? Do we need to augment our model with new information in the prompt (Eg. specific accronyms that are relevant to the user query)

## Monitoring

3. We can see if the specific query planner here is able to generate good queries - do we perhaps need to augment it with something like Splade?

and many more questions

## Integration with other systems

Since our model now outputs a python object that can be serialized to JSON, we can

1. **Do Logging** : Dump the output in something like datadog or logfire for post processing
2. **Integrate it with other existing systems**, just add a requests.post() call to whatever system you're using. This includes whatever python scripts (or agent if u want to use that term) that you have in mind.


## Thank you

Happy to chat more. I'm at @ivanleomk on twitter and my email is ivanleomk@gmail.com.